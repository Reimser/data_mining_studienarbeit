{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hp\\anaconda3\\envs\\crypto_nlp\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"TRANSFORMERS_NO_TF_IMPORT\"] = \"1\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import BertTokenizer, BertForMaskedLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "# Cuda pruefen\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using {} device\".format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìå Datei geladen: G:/Meine Ablage/reddit/reddit_posts_rohdaten.csv\n",
      "üîπ Spalten: ['post_id', 'crypto', 'search_term', 'subreddit', 'title', 'author', 'date', 'time', 'score', 'num_comments', 'selftext']\n",
      "post_id         object\n",
      "crypto          object\n",
      "search_term     object\n",
      "subreddit       object\n",
      "title           object\n",
      "author          object\n",
      "date            object\n",
      "time            object\n",
      "score            int64\n",
      "num_comments     int64\n",
      "selftext        object\n",
      "dtype: object\n",
      "   post_id   crypto search_term       subreddit  \\\n",
      "0  1j0dz4r  Bitcoin     bitcoin  CryptoCurrency   \n",
      "1  1j0dghm  Bitcoin     bitcoin  CryptoCurrency   \n",
      "2  1j0cx0g  Bitcoin     bitcoin  CryptoCurrency   \n",
      "3  1j0cnlh  Bitcoin     bitcoin  CryptoCurrency   \n",
      "4  1j08i0y  Bitcoin     bitcoin  CryptoCurrency   \n",
      "\n",
      "                                               title               author  \\\n",
      "0  BlackRock Adds Its Bitcoin ETF to Model Portfo...             diwalost   \n",
      "1  x-post: As the free-float of coins is low, is ...        3fkgf9fmd980e   \n",
      "2  Companies Building on the Bitcoin Lightning Ne...            kirtash93   \n",
      "3  Michael Saylor Says 'Sell A Kidney, But Keep T...                KIG45   \n",
      "4  Teaching Bitcoin on Wall Street at $30 in 2012...  rizzobitcoinhistory   \n",
      "\n",
      "         date      time  score  num_comments selftext  \n",
      "0  2025-02-28  18:01:02     13             8      NaN  \n",
      "1  2025-02-28  17:39:22      0             1      NaN  \n",
      "2  2025-02-28  17:16:50     63            33      NaN  \n",
      "3  2025-02-28  17:06:18     34            44      NaN  \n",
      "4  2025-02-28  14:09:15    426            23      NaN  \n",
      "\n",
      "üìå Datei geladen: G:/Meine Ablage/reddit/reddit_comments_rohdaten.csv\n",
      "üîπ Spalten: ['post_id', 'comment_id', 'author', 'date', 'time', 'score', 'selftext']\n",
      "post_id       object\n",
      "comment_id    object\n",
      "author        object\n",
      "date          object\n",
      "time          object\n",
      "score          int64\n",
      "selftext      object\n",
      "dtype: object\n",
      "   post_id comment_id         author        date      time  score  \\\n",
      "0  1j0dz4r    mfafvxb       partymsl  2025-02-28  18:04:39      6   \n",
      "1  1j0dz4r    mfah0xz  coinfeeds-bot  2025-02-28  18:10:08      2   \n",
      "2  1j0dz4r    mfb16ty     DonasAskan  2025-02-28  19:43:55      1   \n",
      "3  1j0dz4r    mfakfdl       Deeujian  2025-02-28  18:26:05     -2   \n",
      "4  1j0dz4r    mfag4zu       diwalost  2025-02-28  18:05:51      1   \n",
      "\n",
      "                                            selftext  \n",
      "0      So... who said institutions are capitulating?  \n",
      "1  tldr; BlackRock, the world's largest asset man...  \n",
      "2             How does this only have 3 upvotes lol?  \n",
      "3  1-2% is nothing for Blackrock but they decided...  \n",
      "4                            They are not, retail is  \n"
     ]
    }
   ],
   "source": [
    "# üìå Pfade zu den CSV-Dateien\n",
    "DRIVE_PATH = \"G:/Meine Ablage/reddit/\"\n",
    "POSTS_CSV = os.path.join(DRIVE_PATH, \"reddit_posts_rohdaten.csv\")\n",
    "COMMENTS_CSV = os.path.join(DRIVE_PATH, \"reddit_comments_rohdaten.csv\")\n",
    "\n",
    "# üîπ **Funktion zum Laden der CSV-Dateien**\n",
    "def load_csv(filepath):\n",
    "    \"\"\"L√§dt eine CSV-Datei mit `|` als Trennzeichen und Debugging-Infos\"\"\"\n",
    "    if not os.path.exists(filepath):\n",
    "        print(f\"‚ùå Datei nicht gefunden: {filepath}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    df = pd.read_csv(filepath, sep=\"|\", encoding=\"utf-8-sig\", on_bad_lines=\"skip\")\n",
    "\n",
    "    print(f\"\\nüìå Datei geladen: {filepath}\")\n",
    "    print(f\"üîπ Spalten: {df.columns.tolist()}\")\n",
    "    print(df.dtypes)\n",
    "    print(df.head())\n",
    "\n",
    "    return df\n",
    "\n",
    "# üìå **Daten laden**\n",
    "df_posts = load_csv(POSTS_CSV)\n",
    "df_comments = load_csv(COMMENTS_CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('2025-02-28', '2024-11-01')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_posts.date.max(), df_posts.date.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Bereinigung abgeschlossen. 334346 Eintr√§ge gespeichert in reddit_cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "# üîÑ 2Ô∏è‚É£ Relevante Spalten extrahieren\n",
    "df_posts[\"full_text\"] = df_posts[\"title\"].fillna(\"\") + \" \" + df_posts[\"selftext\"].fillna(\"\")\n",
    "df_comments[\"full_text\"] = df_comments[\"selftext\"].fillna(\"\")\n",
    "\n",
    "# ‚ùå Entferne leere Zeilen\n",
    "df_posts = df_posts[df_posts[\"full_text\"].str.strip() != \"\"]\n",
    "df_comments = df_comments[df_comments[\"full_text\"].str.strip() != \"\"]\n",
    "\n",
    "# Entferne sehr kurze Texte (< 10 Zeichen) und doppelte Eintr√§ge\n",
    "df_posts = df_posts[df_posts[\"full_text\"].str.len() > 10].drop_duplicates(subset=[\"full_text\"])\n",
    "df_comments = df_comments[df_comments[\"full_text\"].str.len() > 10].drop_duplicates(subset=[\"full_text\"])\n",
    "\n",
    "# üîÑ 3Ô∏è‚É£ Posts & Kommentare kombinieren\n",
    "df = pd.concat([df_posts[[\"full_text\"]], df_comments[[\"full_text\"]]], ignore_index=True)\n",
    "\n",
    "print(f\"‚úÖ Bereinigung abgeschlossen. {len(df)} Eintr√§ge gespeichert in reddit_cleaned.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîπ Stichprobe ziehen (z. B. 20.000 Eintr√§ge)\n",
    "df_sample = df.sample(n=20000, random_state=42)\n",
    "\n",
    "# üîπ In Hugging Face Dataset umwandeln\n",
    "dataset = Dataset.from_pandas(df_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20000/20000 [00:10<00:00, 1847.91 examples/s]\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "c:\\Users\\hp\\anaconda3\\envs\\crypto_nlp\\lib\\site-packages\\transformers\\training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_23212\\2519619016.py:33: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5000' max='5000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5000/5000 51:04, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.653000</td>\n",
       "      <td>2.427955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.392600</td>\n",
       "      <td>2.259322</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Training abgeschlossen. Modell gespeichert!\n"
     ]
    }
   ],
   "source": [
    "# üî† 2Ô∏è‚É£ Tokenizer laden (BERT als Basis f√ºr CryptoBERT)\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Tokenisierung mit Padding & Truncation\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"full_text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True, remove_columns=[\"full_text\"])\n",
    "\n",
    "# üîπ 3Ô∏è‚É£ Masked Language Modeling (MLM) vorbereiten\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15)\n",
    "\n",
    "# üîπ 4Ô∏è‚É£ CryptoBERT-Modell laden (auf BERT-Basis)\n",
    "model = BertForMaskedLM.from_pretrained(\"bert-base-uncased\").to(device)\n",
    "\n",
    "# üîπ 5Ô∏è‚É£ Trainingseinstellungen\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./cryptoBERT-posttrained\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=500,\n",
    "    save_total_limit=2,\n",
    "    push_to_hub=False,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "# üîπ 6Ô∏è‚É£ Trainer definieren\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets,\n",
    "    eval_dataset=tokenized_datasets,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "# üî• 7Ô∏è‚É£ Training starten\n",
    "trainer.train()\n",
    "\n",
    "# üìå 8Ô∏è‚É£ Modell speichern (nach jeder Epoche wird es automatisch gespeichert)\n",
    "trainer.save_model(\"./cryptoBERT-posttrained-final\")\n",
    "print(\"‚úÖ Training abgeschlossen. Modell gespeichert!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Perplexity Score (Original BERT): 8.73\n",
      "üöÄ Perplexity Score (CryptoBERT trainiert): 6.86\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "\n",
    "# üìå Lade das trainierte CryptoBERT-Modell\n",
    "crypto_model_path = \"cryptoBERT-posttrained-final\"\n",
    "crypto_model = BertForMaskedLM.from_pretrained(crypto_model_path).eval()\n",
    "\n",
    "# üìå Lade das urspr√ºngliche BERT-Modell zum Vergleich\n",
    "base_model = BertForMaskedLM.from_pretrained(\"bert-base-uncased\").eval()\n",
    "\n",
    "# üìå Tokenizer laden\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# üìå Beispieltext aus den Reddit-Daten\n",
    "text = \"Bitcoin is pumping üöÄ. The market is looking bullish today.\"\n",
    "\n",
    "# üî† Tokenisierung\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "input_ids = inputs[\"input_ids\"]\n",
    "\n",
    "# üî• Perplexity-Funktion\n",
    "def calculate_perplexity(model, input_ids):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, labels=input_ids)\n",
    "        loss = outputs.loss\n",
    "        ppl = math.exp(loss.item())\n",
    "        return ppl\n",
    "\n",
    "# üìä Berechnung des PPL-Scores f√ºr beide Modelle\n",
    "crypto_ppl = calculate_perplexity(crypto_model, input_ids)\n",
    "base_ppl = calculate_perplexity(base_model, input_ids)\n",
    "\n",
    "print(f\"üìä Perplexity Score (Original BERT): {base_ppl:.2f}\")\n",
    "print(f\"üöÄ Perplexity Score (CryptoBERT trainiert): {crypto_ppl:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForMaskedLM were not initialized from the model checkpoint at ElKulako/cryptobert and are newly initialized: ['lm_head.bias', 'lm_head.decoder.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Perplexity Score (Original BERT): 8.73\n",
      "üìä Perplexity Score (CryptoBERT von Hugging Face - ElKulako): 2166877.11\n",
      "üöÄ Perplexity Score (Unser trainiertes CryptoBERT): 6.86\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "# üìå Modelle direkt von Hugging Face laden\n",
    "base_model = AutoModelForMaskedLM.from_pretrained(\"bert-base-uncased\").eval()\n",
    "crypto_bert_base = AutoModelForMaskedLM.from_pretrained(\"ElKulako/cryptobert\").eval()\n",
    "crypto_trained_model = AutoModelForMaskedLM.from_pretrained(\"cryptoBERT-posttrained-final\").eval()\n",
    "\n",
    "# üìå Tokenizer laden\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# üìå Beispieltext aus Reddit (Krypto-Sprache)\n",
    "text = \"Bitcoin is pumping üöÄ. The market is looking bullish today.\"\n",
    "\n",
    "# üî† Tokenisierung\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "input_ids = inputs[\"input_ids\"]\n",
    "\n",
    "# üî• Perplexity-Funktion\n",
    "def calculate_perplexity(model, input_ids):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, labels=input_ids)\n",
    "        loss = outputs.loss\n",
    "        ppl = math.exp(loss.item())\n",
    "        return ppl\n",
    "\n",
    "# üìä Berechnung des PPL-Scores f√ºr alle Modelle\n",
    "ppl_base = calculate_perplexity(base_model, input_ids)\n",
    "ppl_crypto_bert = calculate_perplexity(crypto_bert_base, input_ids)\n",
    "ppl_trained = calculate_perplexity(crypto_trained_model, input_ids)\n",
    "\n",
    "print(f\"üìä Perplexity Score (Original BERT): {ppl_base:.2f}\")\n",
    "print(f\"üìä Perplexity Score (CryptoBERT von Hugging Face - ElKulako): {ppl_crypto_bert:.2f}\")\n",
    "print(f\"üöÄ Perplexity Score (Unser trainiertes CryptoBERT): {ppl_trained:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at ElKulako/cryptobert and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Modell erfolgreich geladen: ElKulako/cryptobert\n",
      "üîç Modellarchitektur: ['RobertaForSequenceClassification']\n",
      "üî¢ Anzahl der Modellparameter: 124645632\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "# üìå Modell & Tokenizer laden\n",
    "MODEL_NAME = \"ElKulako/cryptobert\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModel.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# üìå Modellarchitektur √ºberpr√ºfen\n",
    "print(f\"‚úÖ Modell erfolgreich geladen: {MODEL_NAME}\")\n",
    "print(f\"üîç Modellarchitektur: {model.config.architectures}\")\n",
    "print(f\"üî¢ Anzahl der Modellparameter: {model.num_parameters()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crypto_nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
